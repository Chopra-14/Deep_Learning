import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers 
vocab_size=10000 
(x_train,y_train),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=vocab_size) 
print("Training samples:",len(x_train)) 
print("Test samples:",len(x_test)) 
print("Example review (as integers):",x_train[0][:10]) 
maxlen = 200 
x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) 
x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) 
 
print("Shape of x_train:", x_train.shape) 
print("Shape of x_test:", x_test.shape) 
model=keras.Sequential([ 
    layers.Embedding(input_dim=vocab_size,output_dim=16,input_length=maxlen), 
    layers.GlobalAveragePooling1D(),  #average all embeddings 
    layers.Dense(16,activation='relu'), 
    layers.Dense(1,activation='sigmoid')   #binary output (positive or negative) 
]) 
 
model.summary() 
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 
 
history=model.fit(x_train,y_train, 
                  epochs=5, 
                  batch_size=512, 
                  validation_split=0.2,verbose=1) 
results=model.evaluate(x_test,y_test,verbose=2) 
print("\nTest Accuracy:",results[1]) 
embedding_layer=model.layers[0] 
embeddings=embedding_layer.get_weights()[0] 
print("\nEmbedding matrix shape:",embeddings.shape) 
#Code to Check the Embedding for Any word 
from tensorflow.keras.datasets import imdb 

 
#Load the IMDB word index dictionary 
word_index=imdb.get_word_index() 
reverse_word_index={v+3:k for k,v in word_index.items()} 
reverse_word_index[0]="<PAD>" 
reverse_word_index[1]="<START>" 
reverse_word_index[2]="<UNK>" 
reverse_word_index[3]="<UNUSED>" 
word="college" 
index=word_index.get(word) 
 
if index is not None and index +3 < embeddings.shape[0]: 
  print(f"Word: {word}") 
  print(f"Index in vocabulary:{index+3}") 
  print("Embedding vector:\n",embeddings[index+3]) 
else: 
  priint(f"'{word}' not found in the vocabulary (maybe too rare).") 
